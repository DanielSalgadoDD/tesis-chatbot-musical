{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009306cc",
   "metadata": {},
   "source": [
    "## Tesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import faiss\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434cf0ac",
   "metadata": {},
   "source": [
    "### 1) Tratamiento de las letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2080d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = pd.read_csv('canciones_ingles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8535d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar stopwords de NLTK (solo necesario la primera vez)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar modelo de spaCy para inglés\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39562c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    texto = BeautifulSoup(texto, \"html.parser\").get_text()\n",
    "    texto = re.sub(r\"[^a-zA-Z\\s]\", \"\", texto)  # solo letras y espacios\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto)  # quita saltos de línea, espacios dobles\n",
    "    return texto.lower().strip()\n",
    "\n",
    "\n",
    "def tokenizar_texto(texto):\n",
    "    if isinstance(texto, str):\n",
    "        return texto.split()\n",
    "    else:\n",
    "        return []\n",
    "# Tokenización básica por espacios\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def eliminar_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def lematizar_texto(tokens, nlp):\n",
    "    if not isinstance(tokens, list):\n",
    "        return \"\"\n",
    "    \n",
    "    texto = \" \".join(tokens)  # Convertir lista de tokens a string\n",
    "    doc = nlp(texto)\n",
    "    lemas = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return lemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ojo: estas funciones exigen un gasto computacional alto, tanto de recursos como de tiempo (tenerlo presente antes de correrlo)\n",
    "\n",
    "df_en[\"clean_lyrics\"] = df_en[\"lyrics\"].progress_apply(limpiar_texto)\n",
    "df_en[\"tokens\"] = df_en[\"clean_lyrics\"].progress_apply(tokenizar_texto)\n",
    "df_en[\"tokens_filtrados\"] = df_en[\"tokens\"].progress_apply(eliminar_stopwords)\n",
    "df_en[\"lematizado\"] = df_en[\"tokens_filtrados\"].progress_apply(lambda x: lematizar_texto(x, nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99906a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.to_csv(\"letras_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2e906",
   "metadata": {},
   "source": [
    "### 2) Vector de emociones y embeddings\n",
    "(la base de datos necesaria para realizar las recomendaciones, aún no esta completa, por ende en esta sección se incluye todo lo que se va agregando a la base y su tratamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7244c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones = pd.read_csv(\"letras_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9eca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "    return_all_scores=True,\n",
    "    framework=\"pt\"  \n",
    ")\n",
    "def obtener_vector_emociones(texto):\n",
    "    resultado = classifier(texto[:512])  # Limitamos a 512 tokens por eficiencia\n",
    "    emociones = {item['label']: item['score'] for item in resultado[0]}\n",
    "    return emociones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ojo: esta función exigen un gasto computacional alto y de memoria, tanto de recursos como de tiempo (tenerlo presente antes de correrlo)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "canciones[\"vector_emociones\"] = canciones[\"clean_lyrics\"].progress_apply(obtener_vector_emociones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28091e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte cada diccionario en un vector siguiendo el orden fijo\n",
    "canciones[\"emociones_vector\"] = canciones[\"vector_emociones\"].apply(\n",
    "    lambda d: np.array([d[e] for e in emociones_orden])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e79779",
   "metadata": {},
   "source": [
    "IMPORTANTE: las emociones dadas por el modelo preentrenado vienen en este orden = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4858da",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_embeddings = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\n",
    "\n",
    "canciones['texto_lemmatizado'] = canciones['lematizado'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "tqdm.pandas() \n",
    "canciones['embeddings'] = canciones['texto_lemmatizado'].progress_apply(lambda x: modelo_embeddings.encode(x))\n",
    "\n",
    "# print(len(canciones['embeddings'][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones.to_csv(\"canciones_con_embeddings_completas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee313fa",
   "metadata": {},
   "source": [
    "### 3) Creación de la valencia textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff49bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones = pd.read_csv(\"canciones_con_embeddings_completas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# --- Cargar modelo de sentimiento ---\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# --- Mapeo automático de etiquetas ---\n",
    "id2label = model.config.id2label\n",
    "label2id = model.config.label2id\n",
    "label_names = [id2label[i].lower() for i in range(len(id2label))]\n",
    "\n",
    "if set(label_names) == {\"label_0\", \"label_1\", \"label_2\"}:\n",
    "    # Asumimos: 0=neg, 1=neu, 2=pos\n",
    "    idx_neg, idx_neu, idx_pos = 0, 1, 2\n",
    "else:\n",
    "    idx_neg = label2id.get(\"negative\", 0)\n",
    "    idx_neu = label2id.get(\"neutral\", 1)\n",
    "    idx_pos = label2id.get(\"positive\", 2)\n",
    "\n",
    "def _triple_from_out(out: List[Dict[str, Any]]) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Convierte la salida del pipeline en un triple (neg, neu, pos).\n",
    "    \"\"\"\n",
    "    idx2score = {item[\"label\"]: item[\"score\"] for item in out}\n",
    "    def get_score(idx): \n",
    "        return list(idx2score.values())[idx] if isinstance(idx, int) else idx2score.get(idx, 0.0)\n",
    "    return get_score(idx_neg), get_score(idx_neu), get_score(idx_pos)\n",
    "\n",
    "def infer_and_compute_valence(\n",
    "    texts: List[str],\n",
    "    batch: int = 32\n",
    ") -> Tuple[List[float], np.ndarray, List[Any]]:\n",
    "    \"\"\"\n",
    "    Procesa una lista de textos y devuelve:\n",
    "      - valences: lista de valores valence = p_pos - p_neg\n",
    "      - probs: matriz Nx3 con (neg, neu, pos)\n",
    "      - raw_out: salida cruda del pipeline\n",
    "    \"\"\"\n",
    "    all_probs = []\n",
    "    valences = []\n",
    "    raw_outs = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch), desc=\"Procesando lotes\"):\n",
    "        batch_texts = texts[i:i + batch]\n",
    "        outs = pipe(batch_texts, truncation=True, padding=True)\n",
    "\n",
    "        if isinstance(outs, dict):\n",
    "            outs = [outs]\n",
    "\n",
    "        raw_outs.extend(outs)\n",
    "\n",
    "        for out in outs:\n",
    "            p_neg, p_neu, p_pos = _triple_from_out(out)\n",
    "            all_probs.append([p_neg, p_neu, p_pos])\n",
    "            valences.append(p_pos - p_neg)\n",
    "\n",
    "    return valences, np.array(all_probs), raw_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e15a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = canciones[\"clean_lyrics\"]\n",
    "texts = list(map(str, texts))  # Asegura lista de strings\n",
    "max_length = 512\n",
    "texts = [text[:max_length] for text in texts]\n",
    "\n",
    "valences, probs, info = infer_and_compute_valence(texts, batch=32)\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'valence': valences,\n",
    "    'p_neg': probs[:, 0],\n",
    "    'p_neu': probs[:, 1],\n",
    "    'p_pos': probs[:, 2]\n",
    "})\n",
    "\n",
    "canciones['valence'] = df['valence']\n",
    "canciones['p_neg'] = df['p_neg']\n",
    "canciones['p_neu'] = df['p_neu']\n",
    "canciones['p_pos'] = df['p_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones.to_csv('canciones_con_valencia.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b039d",
   "metadata": {},
   "source": [
    "### 4) Algunas correciones \n",
    "\n",
    "Para que la base de datos quedé ajustada fue necesario realizar algunas correciones \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04013cf",
   "metadata": {},
   "source": [
    " - Reañadimos valencia musical a la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones = pd.read_csv(\"canciones_con_valencia.csv\")\n",
    "canciones2 = pd.read_csv(\"canciones_ingles.csv\")\n",
    "canciones['valence_musical'] = canciones2['valence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393afa75",
   "metadata": {},
   "source": [
    "- Codificamos variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Diccionario para notas musicales → números (0 a 11)\n",
    "key_map = {\n",
    "    'C': 0, 'C#': 1, 'D': 2, 'D#': 3, 'E': 4, 'F': 5,\n",
    "    'F#': 6, 'G': 7, 'G#': 8, 'A': 9, 'A#': 10, 'B': 11\n",
    "}\n",
    "\n",
    "def convert_key(x):\n",
    "    if isinstance(x, str):\n",
    "        # Si es texto que representa número\n",
    "        if x.isdigit():\n",
    "            return int(x)\n",
    "        # Si es nota musical\n",
    "        elif x in key_map:\n",
    "            return key_map[x]\n",
    "    return x  # Dejar valores ya numéricos como están\n",
    "\n",
    "canciones['key'] = canciones['key'].apply(convert_key)\n",
    "\n",
    "# Para 'mode', igual: manejar texto y números\n",
    "mode_map = {'minor': 0, 'major': 1}\n",
    "canciones['mode'] = canciones['mode'].apply(lambda x: mode_map[x.lower()] if isinstance(x, str) and x.lower() in mode_map else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7babb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_br = canciones[['id', 'name', 'album_name','texto_lemmatizado','duration_ms','emociones_vector','valence', 'valence_musical']]\n",
    "canciones_br.to_csv(\"canciones_br.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2fb01b",
   "metadata": {},
   "source": [
    "### 5)  Recomendaciones pt1 (Fue prueba, no significo para la base de recomendaciones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0546014",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_br = pd.read_csv('canciones_br.csv')\n",
    "canciones = pd.read_csv(\"canciones_ingles.csv\")\n",
    "canciones_br['artists'] = canciones['artists']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3254982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_vector(v):\n",
    "    if isinstance(v, str):\n",
    "        # Quitar corchetes y espacios extra\n",
    "        v = v.strip().replace('[', '').replace(']', '')\n",
    "        # Separar por espacios (hay dobles espacios a veces)\n",
    "        parts = v.split()\n",
    "        return np.array([float(x) for x in parts], dtype=np.float32)\n",
    "    elif isinstance(v, (list, tuple, np.ndarray)):\n",
    "        return np.array(v, dtype=np.float32)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Ejemplo de uso en tu DataFrame\n",
    "canciones_br['emociones_vector'] = canciones_br['emociones_vector'].apply(parse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros (ajusta según tu máquina)\n",
    "EMBED_COL = 'emociones_vector'   # columna con los vectores\n",
    "ID_COL = 'id'                    # columna con el id de la canción\n",
    "K = 5                            # queremos las 5 más cercanas\n",
    "BATCH_CONVERT = 20000            # batch para convertir a numpy (evita picos de memoria)\n",
    "BATCH_SEARCH = 20000             # batch para hacer búsquedas (evita picos)\n",
    "USE_INDEXFLAT = True             # intenta IndexFlatIP primero; si falla, cae a IVF\n",
    "INDEX_PATH = \"faiss_emociones.index\"\n",
    "OUT_DF_PATH = \"df_con_vecinos.parquet\"\n",
    "\n",
    "# --- 1) Asegúrate de tener el DataFrame 'df' cargado ---\n",
    "# df = pd.read_parquet(\"...\")  # o como lo tengas cargado\n",
    "\n",
    "# ---- 2) Convertir la columna de vectores a numpy.ndarray dtype=float32 ----\n",
    "def to_array_safe(x):\n",
    "    # Si ya es array o list, convertir; si es string, eval() o parsearlo\n",
    "    if isinstance(x, str):\n",
    "        # intenta eval (seguro si el formato es \"[0.1, 0.2, ...]\" o \"array([...])\")\n",
    "        try:\n",
    "            arr = eval(x)\n",
    "        except Exception:\n",
    "            # como fallback, intentar remover corchetes y split\n",
    "            s = x.strip().lstrip('[').rstrip(']')\n",
    "            parts = [p for p in s.split() if p not in [',']]\n",
    "            arr = [float(p.replace(',','')) for p in parts]\n",
    "        return np.array(arr, dtype=np.float32)\n",
    "    elif isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return np.array(x, dtype=np.float32)\n",
    "    else:\n",
    "        # si hay NaN u otro, devolver vector nan (se manejará)\n",
    "        return np.array([], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en batches para no explotar RAM\n",
    "n = len(canciones_br)\n",
    "sample_vec = None\n",
    "vectors_list = []\n",
    "idx_chunks = list(range(0, n, BATCH_CONVERT))\n",
    "\n",
    "for start in tqdm(idx_chunks, desc=\"Convirtiendo vectors a numpy (batches)\"):\n",
    "    end = min(start + BATCH_CONVERT, n)\n",
    "    chunk = canciones_br[EMBED_COL].iloc[start:end].values\n",
    "    converted = []\n",
    "    for v in chunk:\n",
    "        arr = to_array_safe(v)\n",
    "        converted.append(arr)\n",
    "    # Verificar dimensión y pad / validar\n",
    "    for i, arr in enumerate(converted):\n",
    "        if arr.size == 0:\n",
    "            raise ValueError(f\"Vector vacío en fila {start + i}. Revisa la columna {EMBED_COL}.\")\n",
    "        if sample_vec is None:\n",
    "            sample_vec = arr\n",
    "            dim = arr.shape[0]\n",
    "        else:\n",
    "            if arr.shape[0] != dim:\n",
    "                raise ValueError(f\"Dimensión inconsistente en fila {start + i}: {arr.shape[0]} vs {dim}\")\n",
    "    vectors_list.append(np.vstack(converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar (esto produce la matriz completa; si no entra en RAM habría que procesar con índice incremental)\n",
    "vectors = np.vstack(vectors_list).astype(np.float32)  # shape (n, d)\n",
    "del vectors_list\n",
    "\n",
    "# ---- 3) Normalizar vectores (para usar inner product como coseno) ----\n",
    "vectors = normalize(vectors, axis=1).astype(np.float32)\n",
    "\n",
    "d = vectors.shape[1]\n",
    "print(f\"Num vectores: {vectors.shape[0]}, Dimensión: {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ac6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4) Crear índice FAISS ----\n",
    "index = None\n",
    "try:\n",
    "    if USE_INDEXFLAT:\n",
    "        print(\"Creando IndexFlatIP (exacto, inner product)...\")\n",
    "        index = faiss.IndexFlatIP(d)   # inner product; con vectores normalizados => coseno\n",
    "        # Añadir por batches (si quieres evitar copia grande)\n",
    "        for start in tqdm(range(0, n, BATCH_CONVERT), desc=\"Añadiendo vectores al índice (batches)\"):\n",
    "            end = min(start + BATCH_CONVERT, n)\n",
    "            index.add(vectors[start:end])\n",
    "    else:\n",
    "        raise MemoryError(\"Forzando fallback a IVF\")\n",
    "except Exception as e:\n",
    "    print(\"IndexFlatIP falló o lo forzamos. Usaremos IndexIVFFlat (requiere training).\")\n",
    "    # Fallback: IndexIVFFlat\n",
    "    nlist = int(math.sqrt(n))  # regla práctica: sqrt(n) centroids\n",
    "    quantizer = faiss.IndexFlatIP(d)\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "    # Entrenar con una muestra aleatoria\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    sample_size = min(100000, n)\n",
    "    sample_idx = rng.choice(n, sample_size, replace=False)\n",
    "    print(f\"Entrenando IVF con {sample_size} vectores...\")\n",
    "    index.train(vectors[sample_idx])\n",
    "    # Añadir en batches\n",
    "    for start in tqdm(range(0, n, BATCH_CONVERT), desc=\"Añadiendo vectores al índice IVF (batches)\"):\n",
    "        end = min(start + BATCH_CONVERT, n)\n",
    "        index.add(vectors[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5) Buscar k+1 vecinos (para luego quitar la propia canción) en batches con tqdm ----\n",
    "k_search = K + 1\n",
    "all_indices = np.empty((0, k_search), dtype=np.int64)\n",
    "all_distances = np.empty((0, k_search), dtype=np.float32)\n",
    "\n",
    "for start in tqdm(range(0, n, BATCH_SEARCH), desc=\"Buscando vecinos (batches)\"):\n",
    "    end = min(start + BATCH_SEARCH, n)\n",
    "    q = vectors[start:end]\n",
    "    distances_batch, indices_batch = index.search(q, k_search)   # distances: inner product (similitud)\n",
    "    all_indices = np.vstack((all_indices, indices_batch))\n",
    "    all_distances = np.vstack((all_distances, distances_batch))\n",
    "\n",
    "# ---- 6) Para cada fila, quitar el propio índice y quedarnos con K vecinos ----\n",
    "# indices retornan indices en el orden del DataFrame (0..n-1)\n",
    "nearest_indices = np.zeros((n, K), dtype=np.int64)\n",
    "nearest_sim = np.zeros((n, K), dtype=np.float32)\n",
    "\n",
    "for i in range(n):\n",
    "    inds = all_indices[i]\n",
    "    sims = all_distances[i]\n",
    "    # quitar el mismo elemento (puede no ser el primero si hay exact duplicates)\n",
    "    mask = inds != i\n",
    "    inds_filtered = inds[mask]\n",
    "    sims_filtered = sims[mask]\n",
    "    # si por alguna razón quedan menos de K, rellenar con -1 / 0\n",
    "    if len(inds_filtered) < K:\n",
    "        # rellenar\n",
    "        pad = K - len(inds_filtered)\n",
    "        inds_filtered = np.concatenate([inds_filtered, np.full(pad, -1, dtype=np.int64)])\n",
    "        sims_filtered = np.concatenate([sims_filtered, np.zeros(pad, dtype=np.float32)])\n",
    "    nearest_indices[i, :] = inds_filtered[:K]\n",
    "    nearest_sim[i, :] = sims_filtered[:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10560e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 7) Mapear índices a IDs reales y crear columnas en el DataFrame ----\n",
    "ids_array = canciones_br[ID_COL].values\n",
    "\n",
    "for k in range(K):\n",
    "    col_id = f'cancion_mas_cercana#{k+1}'\n",
    "    col_sim = f'similitud_coseno#{k+1}'\n",
    "    col_dist = f'distancia_coseno#{k+1}'\n",
    "\n",
    "    neighbor_ids_vec = [\n",
    "        ids_array[i] if (i != -1 and i < len(ids_array)) else None\n",
    "        for i in nearest_indices[:, k]\n",
    "    ]\n",
    "    neighbor_sims_vec = [\n",
    "        float(s) if not np.isnan(s) else 0.0\n",
    "        for s in nearest_sim[:, k]\n",
    "    ]\n",
    "    neighbor_dists_vec = [1.0 - s for s in neighbor_sims_vec]\n",
    "\n",
    "    canciones_br[col_id] = neighbor_ids_vec\n",
    "    canciones_br[col_sim] = neighbor_sims_vec\n",
    "    canciones_br[col_dist] = neighbor_dists_vec\n",
    "\n",
    "# ---- 8) Guardar resultados e índice ----\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Convertir emociones_vector a string JSON\n",
    "if isinstance(canciones_br['emociones_vector'].iloc[0], (list, np.ndarray)):\n",
    "    canciones_br['emociones_vector'] = canciones_br['emociones_vector'].apply(\n",
    "        lambda x: json.dumps(x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "    )\n",
    "\n",
    "# Convertir columnas tipo Period a string (solución al ArrowKeyError)\n",
    "for col in canciones_br.columns:\n",
    "    if pd.api.types.is_period_dtype(canciones_br[col]):\n",
    "        canciones_br[col] = canciones_br[col].astype(str)\n",
    "\n",
    "# Convertir columnas de vecinos si contienen objetos raros\n",
    "for col in canciones_br.columns:\n",
    "    if any(keyword in col for keyword in ['cancion_mas_cercana', 'similitud_coseno', 'distancia_coseno']):\n",
    "        canciones_br[col] = canciones_br[col].apply(\n",
    "            lambda x: x if isinstance(x, (float, int, str)) or x is None else str(x)\n",
    "        )\n",
    "\n",
    "print(\"Guardando índice FAISS y DataFrame resultante...\")\n",
    "faiss.write_index(index, INDEX_PATH)\n",
    "canciones_br.to_parquet(OUT_DF_PATH, index=False, engine=\"fastparquet\")\n",
    "\n",
    "print(\"Listo. Columnas añadidas:\")\n",
    "print([c for c in canciones_br.columns if 'cancion_mas_cercana' in c or 'distancia_coseno' in c or 'similitud_coseno' in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85905a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_br = canciones_br.drop(columns=[col for col in canciones_br.columns if col.startswith('distancia')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión robusta de la columna 'emociones_vector' a una matriz numpy homogénea\n",
    "def parse_vector(v):\n",
    "    if isinstance(v, str):\n",
    "        # Si la fila es string, la convertimos a lista\n",
    "        return np.fromstring(v.strip(\"[]\"), sep=' ', dtype='float32')\n",
    "    elif isinstance(v, list):\n",
    "        return np.array(v, dtype='float32')\n",
    "    elif isinstance(v, np.ndarray):\n",
    "        return v.astype('float32')\n",
    "    else:\n",
    "        return np.zeros(6, dtype='float32')  # fallback\n",
    "\n",
    "# Aplicamos la conversión a todas las filas (con tqdm para progreso)\n",
    "X_embeddings = np.vstack([parse_vector(v) for v in tqdm(canciones_br['emociones_vector'].values)])\n",
    "\n",
    "print(f\"Dimensión final: {X_embeddings.shape}\")  # debería ser (709497, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Creamos el índice FAISS\n",
    "index = faiss.IndexFlatIP(X_embeddings.shape[1])  # IP = inner product (para coseno)\n",
    "faiss.normalize_L2(X_embeddings)\n",
    "index.add(X_embeddings)\n",
    "\n",
    "# 3. Definimos los umbrales de diferencia\n",
    "umbral_valence = 0.1          # puedes ajustarlo\n",
    "umbral_valence_musical = 0.1  # puedes ajustarlo\n",
    "\n",
    "# 4. Para cada canción, encontramos las más similares considerando los filtros\n",
    "top_k = 100  # candidatos por FAISS antes de filtrar\n",
    "resultados = []\n",
    "\n",
    "for i in tqdm(range(len(canciones_br)), desc=\"Calculando similitudes filtradas\"):\n",
    "    # Embedding de la canción actual\n",
    "    query = X_embeddings[i].reshape(1, -1)\n",
    "    \n",
    "    # Buscar vecinos más cercanos\n",
    "    D, I = index.search(query, top_k)\n",
    "\n",
    "    # Obtener valores de referencia\n",
    "    val_ref = canciones_br.loc[i, \"valence\"]\n",
    "    val_mus_ref = canciones_br.loc[i, \"valence_musical\"]\n",
    "\n",
    "    # Filtrar por diferencias en valence y valence_musical\n",
    "    candidatos = []\n",
    "    for idx, sim in zip(I[0], D[0]):\n",
    "        if idx == i:  # evitar la misma canción\n",
    "            continue\n",
    "        val = canciones_br.loc[idx, \"valence\"]\n",
    "        val_mus = canciones_br.loc[idx, \"valence_musical\"]\n",
    "\n",
    "        if (abs(val - val_ref) <= umbral_valence) and \\\n",
    "           (abs(val_mus - val_mus_ref) <= umbral_valence_musical):\n",
    "            candidatos.append((idx, sim))\n",
    "\n",
    "    # Ordenar por similitud descendente\n",
    "    candidatos = sorted(candidatos, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    resultados.append({\n",
    "        \"song_index\": i,\n",
    "        \"similares_filtrados\": [idx for idx, _ in candidatos]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_br['canciones_similares_filtradas'] = [r['similares_filtrados'] for r in resultados]\n",
    "canciones_br.to_csv(\"canciones_filtradas.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d1d42",
   "metadata": {},
   "source": [
    "### 6) Añadir emociones mezcladas para posteriores usos en la recomendación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_a_recomendar[\"emociones_vector\"] = canciones_a_recomendar[\"emociones_vector\"].apply(\n",
    "    lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orden de emociones\n",
    "emociones_orden = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "# Diccionario de etiquetas\n",
    "mapping = {\n",
    "    # --- singles ---\n",
    "    (\"anger\",): \"ira\",\n",
    "    (\"fear\",): \"miedo\",\n",
    "    (\"joy\",): \"alegría\",\n",
    "    (\"love\",): \"amor\",\n",
    "    (\"sadness\",): \"tristeza\",\n",
    "    (\"surprise\",): \"sorpresa\",\n",
    "\n",
    "    # --- pares ---\n",
    "    (\"anger\", \"fear\"): \"frustración\",\n",
    "    (\"anger\", \"joy\"): \"ironía alegre\",\n",
    "    (\"anger\", \"love\"): \"pasión conflictiva\",\n",
    "    (\"anger\", \"sadness\"): \"amargura\",\n",
    "    (\"anger\", \"surprise\"): \"ira súbita\",\n",
    "    (\"fear\", \"joy\"): \"euforia ansiosa\",\n",
    "    (\"fear\", \"love\"): \"amor inseguro\",\n",
    "    (\"fear\", \"sadness\"): \"desesperanza\",\n",
    "    (\"fear\", \"surprise\"): \"miedo repentino\",\n",
    "    (\"joy\", \"love\"): \"felicidad romántica\",\n",
    "    (\"joy\", \"sadness\"): \"alegría agridulce\",\n",
    "    (\"joy\", \"surprise\"): \"sorpresa alegre\",\n",
    "    (\"love\", \"sadness\"): \"desamor\",\n",
    "    (\"love\", \"surprise\"): \"amor sorprendente\",\n",
    "    (\"sadness\", \"surprise\"): \"desconcierto triste\",\n",
    "\n",
    "    # --- tríos ---\n",
    "    (\"anger\", \"fear\", \"joy\"): \"ansiedad exaltada\",\n",
    "    (\"anger\", \"fear\", \"love\"): \"amor tormentoso\",\n",
    "    (\"anger\", \"fear\", \"sadness\"): \"desesperación furiosa\",\n",
    "    (\"anger\", \"fear\", \"surprise\"): \"pánico violento\",\n",
    "    (\"anger\", \"joy\", \"love\"): \"pasión eufórica\",\n",
    "    (\"anger\", \"joy\", \"sadness\"): \"contraste emocional\",\n",
    "    (\"anger\", \"joy\", \"surprise\"): \"explosión emocional\",\n",
    "    (\"anger\", \"love\", \"sadness\"): \"relación tóxica\",\n",
    "    (\"anger\", \"love\", \"surprise\"): \"pasión inesperada\",\n",
    "    (\"anger\", \"sadness\", \"surprise\"): \"tristeza explosiva\",\n",
    "    (\"fear\", \"joy\", \"love\"): \"amor ansioso\",\n",
    "    (\"fear\", \"joy\", \"sadness\"): \"esperanza frágil\",\n",
    "    (\"fear\", \"joy\", \"surprise\"): \"alegría nerviosa\",\n",
    "    (\"fear\", \"love\", \"sadness\"): \"amor frágil\",\n",
    "    (\"fear\", \"love\", \"surprise\"): \"amor sorpresivo\",\n",
    "    (\"fear\", \"sadness\", \"surprise\"): \"angustia sorpresiva\",\n",
    "    (\"joy\", \"love\", \"sadness\"): \"nostalgia romántica\",\n",
    "    (\"joy\", \"love\", \"surprise\"): \"felicidad inesperada\",\n",
    "    (\"joy\", \"sadness\", \"surprise\"): \"melancolía alegre\",\n",
    "    (\"love\", \"sadness\", \"surprise\"): \"desamor inesperado\",\n",
    "    # Los demás tríos se cubrirán con fallback automático\n",
    "}\n",
    "\n",
    "def classify_vector(vec, threshold=0.10, max_k=3):\n",
    "    \"\"\"\n",
    "    vec: np.array con 6 probabilidades en orden de emociones_orden\n",
    "    \"\"\"\n",
    "    prob_dict = dict(zip(emociones_orden, vec))\n",
    "\n",
    "    # 1) Filtrar ≥ threshold\n",
    "    selected = [(emo, p) for emo, p in prob_dict.items() if p >= threshold]\n",
    "\n",
    "    if not selected:  \n",
    "        # Fallback → top1\n",
    "        top1 = max(prob_dict.items(), key=lambda x: x[1])[0]\n",
    "        key = (top1,)\n",
    "    else:\n",
    "        # Ordenar por prob desc y tomar hasta max_k\n",
    "        selected_sorted = sorted(selected, key=lambda x: x[1], reverse=True)[:max_k]\n",
    "        emos = [e for e,_ in selected_sorted]\n",
    "        key = tuple(sorted(emos))  # canonicalizar (orden alfabético)\n",
    "\n",
    "    # Buscar etiqueta en mapping\n",
    "    label = mapping.get(key, \" + \".join(key))  # fallback automático\n",
    "    return label\n",
    "\n",
    "# Aplicar al dataframe\n",
    "canciones_a_recomendar[\"emocion_label\"] = canciones_a_recomendar[\"emociones_vector\"].apply(\n",
    "    lambda v: classify_vector(np.array(eval(v)) if isinstance(v, str) else np.array(v))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1635b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_a_recomendar.to_csv('canciones_filtradas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54821a",
   "metadata": {},
   "source": [
    "### 7) Creación de la base de recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_a_recomendar = pd.read_csv(\"canciones_flitradas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4cd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = canciones_a_recomendar.copy()\n",
    "\n",
    "# Si la columna es texto tipo \"[0.63 0.01 ...]\" la convertimos a lista\n",
    "def parse_vector(x):\n",
    "    if isinstance(x, str):\n",
    "        # Quitar corchetes y convertir a floats\n",
    "        x = x.strip(\"[]\").split()\n",
    "        return np.array(x, dtype=np.float32)\n",
    "    elif isinstance(x, (list, np.ndarray)):\n",
    "        return np.array(x, dtype=np.float32)\n",
    "    else:\n",
    "        return np.array([], dtype=np.float32)  # Por si hay nulos\n",
    "\n",
    "df['emociones_vector'] = df['emociones_vector'].apply(parse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbfcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_emociones = 0.7\n",
    "w_valence = 0.2\n",
    "w_valence_musical = 0.1\n",
    "\n",
    "# Conversión robusta de 'emociones_vector'\n",
    "def parse_vector(x):\n",
    "    if isinstance(x, str):\n",
    "        return np.fromstring(x.strip(\"[]\"), sep=\" \", dtype=np.float32)\n",
    "    elif isinstance(x, (list, np.ndarray)):\n",
    "        return np.array(x, dtype=np.float32)\n",
    "    else:\n",
    "        return np.array([], dtype=np.float32)\n",
    "\n",
    "df['emociones_vector'] = df['emociones_vector'].apply(parse_vector)\n",
    "\n",
    "# Verificar un ejemplo\n",
    "print(\"Ejemplo convertido:\", df['emociones_vector'].iloc[0])\n",
    "\n",
    "# Construcción de la matriz final\n",
    "X_emociones = np.vstack(df['emociones_vector'].values).astype('float32')\n",
    "valence = df['valence'].astype('float32').values[:, None]\n",
    "valence_musical = df['valence_musical'].astype('float32').values[:, None]\n",
    "\n",
    "X_final = np.hstack([\n",
    "    X_emociones * w_emociones,\n",
    "    valence * w_valence,\n",
    "    valence_musical * w_valence_musical\n",
    "]).astype('float32')\n",
    "\n",
    "# Normalizamos\n",
    "faiss.normalize_L2(X_final)\n",
    "\n",
    "# Creamos índice FAISS\n",
    "index = faiss.IndexFlatIP(X_final.shape[1])\n",
    "index.add(X_final)\n",
    "\n",
    "# Número de vecinos (10 + 1 porque el primero es la misma canción)\n",
    "k = 10 + 1  \n",
    "\n",
    "# Buscamos los vecinos\n",
    "distances, indices = index.search(X_final, k)\n",
    "\n",
    "# Generar DataFrame de recomendaciones\n",
    "recommendations = []\n",
    "for i in range(len(df)):\n",
    "    recs = [(df.iloc[idx]['id'], float(dist)) for idx, dist in zip(indices[i][1:], distances[i][1:])]\n",
    "    recommendations.append(recs)\n",
    "\n",
    "rec_df = pd.DataFrame({\n",
    "    'id': df['id'],\n",
    "    'name': df['name'],\n",
    "    **{f'recom_{j+1}': [recs[j] for recs in recommendations] for j in range(10)}\n",
    "})\n",
    "\n",
    "print(rec_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.to_csv(\"rec_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
